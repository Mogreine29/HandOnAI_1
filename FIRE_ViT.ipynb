{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import Image, HTML, display\nfrom matplotlib import pyplot as plt\nimport numpy as np \nimport os\nimport cv2\nimport csv\n\nimport math\nimport argparse\nimport matplotlib\nimport imghdr\nimport pickle as pkl\nimport datetime\nfrom cycler import cycler\nfrom PIL import Image, ImageEnhance\n\nfrom pathlib import Path\nimport os.path\n\nfrom datasets import load_dataset\nfrom datasets import load_metric\n\nfrom transformers import TrainingArguments\nfrom transformers import ViTFeatureExtractor\nfrom transformers import ViTForImageClassification\n\nimport torch\n\nfrom PIL import Image\nimport requests\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-30T19:46:02.640806Z","iopub.execute_input":"2022-10-30T19:46:02.641221Z","iopub.status.idle":"2022-10-30T19:46:02.648374Z","shell.execute_reply.started":"2022-10-30T19:46:02.641188Z","shell.execute_reply":"2022-10-30T19:46:02.647305Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"!rm -rf sample_data\n!wget https://cluster.ig.umons.ac.be/HackIA21/databases/FIRE_DATABASE_3.tar\n!tar xf FIRE_DATABASE_3.tar\n!rm FIRE_DATABASE_3.tar","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:02.650336Z","iopub.execute_input":"2022-10-30T19:46:02.651358Z","iopub.status.idle":"2022-10-30T19:46:30.167678Z","shell.execute_reply.started":"2022-10-30T19:46:02.651320Z","shell.execute_reply":"2022-10-30T19:46:30.166297Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"--2022-10-30 19:46:04--  https://cluster.ig.umons.ac.be/HackIA21/databases/FIRE_DATABASE_3.tar\nResolving cluster.ig.umons.ac.be (cluster.ig.umons.ac.be)... 193.190.209.220\nConnecting to cluster.ig.umons.ac.be (cluster.ig.umons.ac.be)|193.190.209.220|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 471111680 (449M) [application/octet-stream]\nSaving to: ‘FIRE_DATABASE_3.tar’\n\nFIRE_DATABASE_3.tar 100%[===================>] 449.29M  22.0MB/s    in 22s     \n\n2022-10-30 19:46:27 (20.2 MB/s) - ‘FIRE_DATABASE_3.tar’ saved [471111680/471111680]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Données de test\n!rm -rf sample_data\n!wget --no-check-certificate http://195.154.53.219/downloads/test.tar\n! tar xf test.tar -C 'test_data' --one-top-level\n! rm test.tar","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:30.170043Z","iopub.execute_input":"2022-10-30T19:46:30.170462Z","iopub.status.idle":"2022-10-30T19:46:37.318924Z","shell.execute_reply.started":"2022-10-30T19:46:30.170417Z","shell.execute_reply":"2022-10-30T19:46:37.317564Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"--2022-10-30 19:46:32--  http://195.154.53.219/downloads/test.tar\nConnecting to 195.154.53.219:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 58196480 (56M) [application/octet-stream]\nSaving to: ‘test.tar’\n\ntest.tar            100%[===================>]  55.50M  21.6MB/s    in 2.6s    \n\n2022-10-30 19:46:35 (21.6 MB/s) - ‘test.tar’ saved [58196480/58196480]\n\ntar: test_data: Cannot open: No such file or directory\ntar: Error is not recoverable: exiting now\n","output_type":"stream"}]},{"cell_type":"code","source":"ds = load_dataset(\"imagefolder\", data_dir = \"./FIRE_DATABASE_3\")\nds","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:37.322722Z","iopub.execute_input":"2022-10-30T19:46:37.323171Z","iopub.status.idle":"2022-10-30T19:46:37.887547Z","shell.execute_reply.started":"2022-10-30T19:46:37.323124Z","shell.execute_reply":"2022-10-30T19:46:37.886629Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/1500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"480d73c24d6f4d129ecc0cdb79abfb19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a5a2e82f9740b7aa39d47aea6dc05d"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"data = ds['train'].train_test_split(test_size = 0.1)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:37.889147Z","iopub.execute_input":"2022-10-30T19:46:37.889769Z","iopub.status.idle":"2022-10-30T19:46:37.935642Z","shell.execute_reply.started":"2022-10-30T19:46:37.889733Z","shell.execute_reply":"2022-10-30T19:46:37.934623Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 1350\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 150\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = data['train'].features['label']\nlabels = data[\"train\"].features[\"label\"].names\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:37.937176Z","iopub.execute_input":"2022-10-30T19:46:37.937525Z","iopub.status.idle":"2022-10-30T19:46:37.943005Z","shell.execute_reply.started":"2022-10-30T19:46:37.937490Z","shell.execute_reply":"2022-10-30T19:46:37.941896Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"metric = load_metric('accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:37.944576Z","iopub.execute_input":"2022-10-30T19:46:37.944922Z","iopub.status.idle":"2022-10-30T19:46:38.201089Z","shell.execute_reply.started":"2022-10-30T19:46:37.944889Z","shell.execute_reply":"2022-10-30T19:46:38.200198Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:38.202946Z","iopub.execute_input":"2022-10-30T19:46:38.203612Z","iopub.status.idle":"2022-10-30T19:46:38.320006Z","shell.execute_reply.started":"2022-10-30T19:46:38.203577Z","shell.execute_reply":"2022-10-30T19:46:38.319169Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"loading feature extractor configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7c7f3e780b30eeeacd3962294e5154788caa6d9aa555ed6d5c2f0d2c485eba18.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\nFeature extractor ViTFeatureExtractor {\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"size\": 224\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.transforms import (\n    CenterCrop,\n    Compose,\n    Normalize,\n    RandomHorizontalFlip,\n    RandomResizedCrop,\n    Resize,\n    ToTensor,\n)\n\nnormalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\ntrain_transforms = Compose(\n        [\n            RandomResizedCrop(feature_extractor.size),\n            RandomHorizontalFlip(),\n            ToTensor(),\n            normalize,\n        ]\n    )\n\nval_transforms = Compose(\n        [\n            Resize(feature_extractor.size),\n            CenterCrop(feature_extractor.size),\n            ToTensor(),\n            normalize,\n        ]\n    )\n\ndef preprocess_train(example_batch):\n    \"\"\"Apply train_transforms across a batch.\"\"\"\n    example_batch[\"pixel_values\"] = [\n        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n    ]\n    return example_batch\n\ndef preprocess_val(example_batch):\n    \"\"\"Apply val_transforms across a batch.\"\"\"\n    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n    return example_batch","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:38.321191Z","iopub.execute_input":"2022-10-30T19:46:38.321552Z","iopub.status.idle":"2022-10-30T19:46:38.331479Z","shell.execute_reply.started":"2022-10-30T19:46:38.321514Z","shell.execute_reply":"2022-10-30T19:46:38.330522Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_ds = data['train']\nval_ds = data['test']\ntest_ds = load_dataset(\"imagefolder\", data_dir = \"./FIRE_DATABASE_3\")['train']","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:38.336364Z","iopub.execute_input":"2022-10-30T19:46:38.336830Z","iopub.status.idle":"2022-10-30T19:46:39.143337Z","shell.execute_reply.started":"2022-10-30T19:46:38.336724Z","shell.execute_reply":"2022-10-30T19:46:39.142418Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/1500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30aea270b6a24d5d84d670d317d96fe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"decec0956c8e4846b982f792031cad21"}},"metadata":{}}]},{"cell_type":"code","source":"test_ds","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:39.144631Z","iopub.execute_input":"2022-10-30T19:46:39.145582Z","iopub.status.idle":"2022-10-30T19:46:39.152445Z","shell.execute_reply.started":"2022-10-30T19:46:39.145542Z","shell.execute_reply":"2022-10-30T19:46:39.151464Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['image', 'label'],\n    num_rows: 1500\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_ds.set_transform(preprocess_train)\nval_ds.set_transform(preprocess_val)\ntest_ds.set_transform(preprocess_val)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:51:03.148924Z","iopub.execute_input":"2022-10-30T19:51:03.149636Z","iopub.status.idle":"2022-10-30T19:51:03.158669Z","shell.execute_reply.started":"2022-10-30T19:51:03.149596Z","shell.execute_reply":"2022-10-30T19:51:03.157646Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"train_ds[0]","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:39.167437Z","iopub.execute_input":"2022-10-30T19:46:39.168471Z","iopub.status.idle":"2022-10-30T19:46:39.210872Z","shell.execute_reply.started":"2022-10-30T19:46:39.168433Z","shell.execute_reply":"2022-10-30T19:46:39.209928Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x960>,\n 'label': 0,\n 'pixel_values': tensor([[[-0.0980, -0.0980, -0.0745,  ..., -0.0667, -0.0745, -0.0824],\n          [-0.0902, -0.0824, -0.0588,  ..., -0.0588, -0.0667, -0.0745],\n          [-0.0588, -0.0745, -0.0588,  ..., -0.0431, -0.0510, -0.0667],\n          ...,\n          [-0.3020, -0.3569, -0.3961,  ..., -0.5137, -0.4039, -0.2784],\n          [-0.2863, -0.3255, -0.3647,  ..., -0.5294, -0.4353, -0.3176],\n          [-0.2863, -0.3255, -0.3569,  ..., -0.5373, -0.4667, -0.3647]],\n \n         [[-0.7569, -0.7569, -0.7333,  ..., -0.7176, -0.7569, -0.7647],\n          [-0.7569, -0.7490, -0.7255,  ..., -0.7176, -0.7412, -0.7490],\n          [-0.7412, -0.7490, -0.7333,  ..., -0.7176, -0.7255, -0.7412],\n          ...,\n          [-0.8667, -0.8824, -0.9059,  ..., -0.9765, -0.9373, -0.8824],\n          [-0.8510, -0.8667, -0.8902,  ..., -0.9765, -0.9451, -0.8824],\n          [-0.8588, -0.8667, -0.8824,  ..., -0.9843, -0.9529, -0.8824]],\n \n         [[-0.9922, -0.9843, -0.9686,  ..., -0.9922, -1.0000, -0.9843],\n          [-0.9843, -0.9765, -0.9608,  ..., -0.9922, -0.9922, -0.9765],\n          [-0.9765, -0.9843, -0.9765,  ..., -0.9922, -0.9765, -0.9765],\n          ...,\n          [-0.9765, -0.9765, -0.9765,  ..., -0.9922, -0.9843, -0.9922],\n          [-0.9686, -0.9765, -0.9843,  ..., -0.9922, -0.9843, -0.9843],\n          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843]]])}"},"metadata":{}}]},{"cell_type":"code","source":"model_name_or_path = 'google/vit-base-patch16-224-in21k'\nmodel = ViTForImageClassification.from_pretrained(\n    model_name_or_path, \n    num_labels=len(labels),\n    id2label={str(i): c for i, c in enumerate(labels)},\n    label2id={c: str(i) for i, c in enumerate(labels)}\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:39.212322Z","iopub.execute_input":"2022-10-30T19:46:39.212665Z","iopub.status.idle":"2022-10-30T19:46:42.430235Z","shell.execute_reply.started":"2022-10-30T19:46:39.212631Z","shell.execute_reply":"2022-10-30T19:46:42.429320Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/7bba26dd36a6ff9f6a9b19436dec361727bea03ec70fbfa82b70628109163eaa.92995a56e2eabab0c686015c4ad8275b4f9cbd858ed228f6a08936f2c31667e7\nModel config ViTConfig {\n  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"fire\",\n    \"1\": \"no_fire\",\n    \"2\": \"start_fire\"\n  },\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"fire\": \"0\",\n    \"no_fire\": \"1\",\n    \"start_fire\": \"2\"\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"qkv_bias\": true,\n  \"transformers_version\": \"4.20.1\"\n}\n\nloading weights file https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/d01bfc4a52063e6f2cc1bc7063192e012043a7c6d8e75981bb6afbb9dc911001.e4710baf72bd00d091aab2ae692d487c057734cf044ba421696823447b95521e\nSome weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    'finetuned-fire-detection',\n  per_device_train_batch_size=16,\n  evaluation_strategy=\"steps\",\n  num_train_epochs=4,\n  fp16=True,\n  save_steps=100,\n  eval_steps=100,\n  logging_steps=10,\n  learning_rate=2e-4,\n  save_total_limit=2,\n  remove_unused_columns=False,\n  report_to='tensorboard',\n  load_best_model_at_end=True,\n  hub_strategy=\"end\"\n)#","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:42.431673Z","iopub.execute_input":"2022-10-30T19:46:42.432142Z","iopub.status.idle":"2022-10-30T19:46:42.444658Z","shell.execute_reply.started":"2022-10-30T19:46:42.432103Z","shell.execute_reply":"2022-10-30T19:46:42.443744Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:42.447704Z","iopub.execute_input":"2022-10-30T19:46:42.448614Z","iopub.status.idle":"2022-10-30T19:46:42.667270Z","shell.execute_reply.started":"2022-10-30T19:46:42.448581Z","shell.execute_reply":"2022-10-30T19:46:42.666021Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.tensor([x['label'] for x in batch])\n    }","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:42.669228Z","iopub.execute_input":"2022-10-30T19:46:42.670774Z","iopub.status.idle":"2022-10-30T19:46:42.680365Z","shell.execute_reply.started":"2022-10-30T19:46:42.670744Z","shell.execute_reply":"2022-10-30T19:46:42.679446Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics,\n    data_collator=collate_fn,\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:42.681703Z","iopub.execute_input":"2022-10-30T19:46:42.682553Z","iopub.status.idle":"2022-10-30T19:46:42.780989Z","shell.execute_reply.started":"2022-10-30T19:46:42.682510Z","shell.execute_reply":"2022-10-30T19:46:42.780113Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"Using cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"train_results = trainer.train()\n# rest is optional but nice to have\ntorch.save(model.state_dict(), \"model.pth\")\ntrainer.save_model('model.h5')\ntrainer.log_metrics(\"train\", train_results.metrics)\ntrainer.save_metrics(\"train\", train_results.metrics)\ntrainer.save_state()","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:46:42.782332Z","iopub.execute_input":"2022-10-30T19:46:42.782855Z","iopub.status.idle":"2022-10-30T19:50:33.898271Z","shell.execute_reply.started":"2022-10-30T19:46:42.782818Z","shell.execute_reply":"2022-10-30T19:50:33.897128Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 1350\n  Num Epochs = 4\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 340\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [340/340 03:48, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.117600</td>\n      <td>0.070909</td>\n      <td>0.993333</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.132200</td>\n      <td>0.039215</td>\n      <td>0.993333</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.118100</td>\n      <td>0.068777</td>\n      <td>0.980000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 150\n  Batch size = 8\nSaving model checkpoint to finetuned-fire-detection/checkpoint-100\nConfiguration saved in finetuned-fire-detection/checkpoint-100/config.json\nModel weights saved in finetuned-fire-detection/checkpoint-100/pytorch_model.bin\nFeature extractor saved in finetuned-fire-detection/checkpoint-100/preprocessor_config.json\nDeleting older checkpoint [finetuned-fire-detection/checkpoint-200] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 150\n  Batch size = 8\nSaving model checkpoint to finetuned-fire-detection/checkpoint-200\nConfiguration saved in finetuned-fire-detection/checkpoint-200/config.json\nModel weights saved in finetuned-fire-detection/checkpoint-200/pytorch_model.bin\nFeature extractor saved in finetuned-fire-detection/checkpoint-200/preprocessor_config.json\nDeleting older checkpoint [finetuned-fire-detection/checkpoint-300] due to args.save_total_limit\n***** Running Evaluation *****\n  Num examples = 150\n  Batch size = 8\nSaving model checkpoint to finetuned-fire-detection/checkpoint-300\nConfiguration saved in finetuned-fire-detection/checkpoint-300/config.json\nModel weights saved in finetuned-fire-detection/checkpoint-300/pytorch_model.bin\nFeature extractor saved in finetuned-fire-detection/checkpoint-300/preprocessor_config.json\nDeleting older checkpoint [finetuned-fire-detection/checkpoint-100] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from finetuned-fire-detection/checkpoint-200 (score: 0.039215248078107834).\nSaving model checkpoint to model.h5\nConfiguration saved in model.h5/config.json\nModel weights saved in model.h5/pytorch_model.bin\nFeature extractor saved in model.h5/preprocessor_config.json\n","output_type":"stream"},{"name":"stdout","text":"***** train metrics *****\n  epoch                    =         4.0\n  total_flos               = 389721705GF\n  train_loss               =      0.1623\n  train_runtime            =  0:03:49.67\n  train_samples_per_second =      23.512\n  train_steps_per_second   =        1.48\n","output_type":"stream"}]},{"cell_type":"code","source":"metrics = trainer.evaluate()\ntrainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:50:33.902151Z","iopub.execute_input":"2022-10-30T19:50:33.902562Z","iopub.status.idle":"2022-10-30T19:50:38.093861Z","shell.execute_reply.started":"2022-10-30T19:50:33.902515Z","shell.execute_reply":"2022-10-30T19:50:38.092834Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 150\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='207' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19/19 01:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =        4.0\n  eval_accuracy           =     0.9933\n  eval_loss               =     0.0392\n  eval_runtime            = 0:00:04.17\n  eval_samples_per_second =     35.894\n  eval_steps_per_second   =      4.547\n","output_type":"stream"}]},{"cell_type":"code","source":"outputs = trainer.predict(test_ds)\nprint(outputs.metrics)","metadata":{"execution":{"iopub.status.busy":"2022-10-30T19:51:08.917360Z","iopub.execute_input":"2022-10-30T19:51:08.917719Z","iopub.status.idle":"2022-10-30T19:51:52.020316Z","shell.execute_reply.started":"2022-10-30T19:51:08.917687Z","shell.execute_reply":"2022-10-30T19:51:52.019253Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 1500\n  Batch size = 8\n","output_type":"stream"},{"name":"stdout","text":"{'test_loss': 0.04682157561182976, 'test_accuracy': 0.9906666666666667, 'test_runtime': 43.0964, 'test_samples_per_second': 34.806, 'test_steps_per_second': 4.362}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}